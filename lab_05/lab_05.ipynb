{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KzH7lbZ0WFBL"
   },
   "source": [
    "<img align=\"right\" style=\"max-width: 200px; height: auto\" src=\"https://github.com/HSG-AIML/LabAIML/blob/main/lab_05/hsg_logo.png?raw=1\">\n",
    "\n",
    "##  Lab 05 - \"Deep Learning - Convolutional Neural Networks\"\n",
    "\n",
    "Introduction to AI and ML/ML and DL, University of St. Gallen, Fall Term 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DSgCj0tOWFBQ"
   },
   "source": [
    "In the last lab you learned about how to utilize a **supervised** (deep) machine learning technique namely **Artificial Neural Networks (ANNs)** to classify tiny images of fashion articles contained in the FashionMNIST dataset. \n",
    "\n",
    "In this lab, we will learn how to enhance ANNs using PyTorch to classify even more complex images. Therefore, we use a special type of deep neural network referred to **Convolutional Neural Networks (CNNs)**. CNNs encompass the ability to take advantage of the hierarchical pattern in data and assemble more complex patterns using smaller and simpler patterns. Therefore, CNNs are capable to learn a set of discriminative features 'pattern' and subsequently utilize the learned pattern to classify the content of an image.\n",
    "\n",
    "We will again use the functionality of the `PyTorch` library to implement and train an CNN based neural network. The network will be trained on a set of **tiny images of objects** to learn a model of the image content. Upon successful training, we will utilize the learned CNN model to classify so far unseen tiny images into distinct categories such as aeroplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks. \n",
    "\n",
    "The figure below illustrates a high-level view on the machine learning process we aim to establish in this lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qh_q8bQWWFBU"
   },
   "source": [
    "<img align=\"center\" style=\"max-width: 900px\" src=\"https://github.com/HSG-AIML/LabAIML/blob/main/lab_05/classification.png?raw=1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v9yXWnmoWFBU"
   },
   "source": [
    "(Image of the CNN architecture created via http://alexlenail.me/)\n",
    "\n",
    "As always, pls. don't hesitate to ask all your questions either during the lab, post them in our CANVAS (StudyNet) forum (https://learning.unisg.ch), or send us an email (using the course email)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8YfYnXvNWFBV"
   },
   "source": [
    "## 1. Lab Objectives:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GudZC3PJWFBW"
   },
   "source": [
    "After today's lab, you should be able to:\n",
    "\n",
    "> 1. Understand the basic concepts, intuitions and major building blocks of **Convolutional Neural Networks (CNNs)**.\n",
    "> 2. Know how to **implement and to train a CNN** to learn a model of tiny image data.\n",
    "> 3. Understand how to apply such a learned model to **classify images** images based on their content into distinct categories.\n",
    "> 4. Know how to **interpret and visualize** the model's classification results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0NlCbmDLWFBY"
   },
   "source": [
    "## 2. Setup of the Jupyter Notebook Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i1wsfWemWFBY"
   },
   "source": [
    "Suppress potential warnings throughout the notebook execution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "W6naUrD1WFBY"
   },
   "outputs": [],
   "source": [
    "# import the warnings library\n",
    "import warnings\n",
    "\n",
    "# ignore potential library warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U_WtK2L3WFBZ"
   },
   "source": [
    "Similar to the previous labs, we need to import a couple of Python libraries that allow for data analysis and data visualization. We will mostly use the `PyTorch`, `Numpy`, `Sklearn`, `Matplotlib`, `Seaborn` and a few utility libraries throughout this lab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "UsRyll5xWFBZ"
   },
   "outputs": [],
   "source": [
    "# import standard python libraries\n",
    "import os\n",
    "from datetime import datetime\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s5wTjcH4WFBZ"
   },
   "source": [
    "Import Python machine / deep learning libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "EoyY0f4AWFBa"
   },
   "outputs": [],
   "source": [
    "# import the PyTorch deep learning library\n",
    "import torch, torchvision\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "alUNeby2WFBa"
   },
   "source": [
    "Import the sklearn classification metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "oGZv9KxqWFBa"
   },
   "outputs": [],
   "source": [
    "# import sklearn classification evaluation library\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0pIcDk9mWFBb"
   },
   "source": [
    "Import Python plotting libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "O2MO-UwTWFBb"
   },
   "outputs": [],
   "source": [
    "# import matplotlib, seaborn, and PIL data visualization libary\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jgr8HQUGWFBb"
   },
   "source": [
    "Enable notebook matplotlib inline plotting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "iaikyfMLWFBb"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pMh2T4LjWFBc"
   },
   "source": [
    "Create notebook folder structure to store the data as well as the trained neural network models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "On9JJF_GWFBc"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('./data'): os.makedirs('./data')  # create data directory\n",
    "if not os.path.exists('./models'): os.makedirs('./models')  # create trained models directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TjbW_9GOWFBc"
   },
   "source": [
    "Set a random `seed` value to obtain reproducible results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "5HCpoOHMWFBc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f2d836a5e70>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# init deterministic seed\n",
    "seed_value = 42\n",
    "np.random.seed(seed_value) # set numpy seed\n",
    "torch.manual_seed(seed_value) # set pytorch seed CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AMwNx9eOWFBe"
   },
   "source": [
    "## 3. Dataset Download and Data Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KBDHCbK3WFBf"
   },
   "source": [
    "The **CIFAR-10 database** (**C**anadian **I**nstitute **F**or **A**dvanced **R**esearch) is a collection of images that are commonly used to train machine learning and computer vision algorithms. The database is widely used to conduct computer vision research using machine learning and deep learning methods:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J7alUozTWFBf"
   },
   "source": [
    "<img align=\"center\" style=\"max-width: 500px; height: 500px\" src=\"https://github.com/HSG-AIML/LabAIML/blob/main/lab_05/cifar10.png?raw=1\">\n",
    "\n",
    "(Source: https://www.kaggle.com/c/cifar-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gIm_BreZWFBg"
   },
   "source": [
    "Further details on the dataset can be obtained via: *Krizhevsky, A., 2009. \"Learning Multiple Layers of Features from Tiny Images\",  \n",
    "( https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf ).\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MVObUimmWFBg"
   },
   "source": [
    "The CIFAR-10 database contains **60,000 color images** (50,000 training images and 10,000 validation images). The size of each image is 32 by 32 pixels. The collection of images encompasses 10 different classes that represent airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks. Let's define the distinct classs for further analytics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "5teZNMkxWFBg"
   },
   "outputs": [],
   "source": [
    "cifar10_classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8IMla6YSWFBg"
   },
   "source": [
    "Thereby the dataset contains 6,000 images for each of the ten classes. The CIFAR-10 is a straightforward dataset that can be used to teach a computer how to recognize objects in images.\n",
    "\n",
    "Let's download, transform and inspect the training images of the dataset. Therefore, we first will define the directory we aim to store the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "fUZioGfnWFBh"
   },
   "outputs": [],
   "source": [
    "train_path = './data/train_cifar10'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7kIwvdFlWFBh"
   },
   "source": [
    "Now, let's download the training data accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "UKyND-cCWFBh"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/train_cifar10/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 170459136/170498071 [02:27<00:00, 710530.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/train_cifar10/cifar-10-python.tar.gz to ./data/train_cifar10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "170500096it [02:39, 710530.27it/s]                               "
     ]
    }
   ],
   "source": [
    "# define pytorch transformation into tensor format\n",
    "transf = torchvision.transforms.Compose([torchvision.transforms.ToTensor(), torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "# download and transform training images\n",
    "cifar10_train_data = torchvision.datasets.CIFAR10(root=train_path, train=True, transform=transf, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mvd2Eq65WFBk"
   },
   "source": [
    "Verify the volume of training images downloaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "IGlnoc1vWFBl"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the length of the training data\n",
    "len(cifar10_train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yCTj7UKpWFBl"
   },
   "source": [
    "Furthermore, let's investigate a couple of the training images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "-nLP6ILFWFBl"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.0912,  0.0569,  0.0912,  ...,  0.1426,  0.1426,  0.1254],\n",
       "          [ 0.0741,  0.0741,  0.0912,  ...,  0.1254,  0.1254,  0.1083],\n",
       "          [ 0.1083,  0.0741,  0.1083,  ...,  0.1597,  0.1426,  0.1426],\n",
       "          ...,\n",
       "          [-0.3712, -0.5082, -0.9705,  ...,  0.1768,  0.1426,  0.1254],\n",
       "          [-0.2684, -0.4739, -0.7479,  ...,  0.1597,  0.1254,  0.1083],\n",
       "          [-0.2342, -0.7479, -1.0048,  ...,  0.1254,  0.1083,  0.0912]],\n",
       " \n",
       "         [[ 0.6254,  0.5903,  0.6254,  ...,  0.6429,  0.6429,  0.6254],\n",
       "          [ 0.6254,  0.6078,  0.6254,  ...,  0.6254,  0.6254,  0.6078],\n",
       "          [ 0.6429,  0.6078,  0.6429,  ...,  0.6604,  0.6429,  0.6429],\n",
       "          ...,\n",
       "          [-0.0574, -0.2150, -0.8452,  ...,  0.7129,  0.6779,  0.6604],\n",
       "          [ 0.0476, -0.1275, -0.6001,  ...,  0.6954,  0.6604,  0.6429],\n",
       "          [ 0.0651, -0.3550, -0.8277,  ...,  0.6604,  0.6429,  0.6254]],\n",
       " \n",
       "         [[ 1.6291,  1.5768,  1.6117,  ...,  1.6465,  1.6465,  1.6291],\n",
       "          [ 1.6117,  1.5768,  1.6117,  ...,  1.6291,  1.6291,  1.6117],\n",
       "          [ 1.6291,  1.5942,  1.6291,  ...,  1.6640,  1.6465,  1.6465],\n",
       "          ...,\n",
       "          [-1.1073, -0.9678, -1.1421,  ...,  1.6988,  1.6640,  1.6465],\n",
       "          [-0.9853, -0.8633, -0.8981,  ...,  1.6814,  1.6465,  1.6291],\n",
       "          [-0.8284, -1.0550, -1.1247,  ...,  1.6465,  1.6291,  1.6117]]]),\n",
       " 2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set (random) image id\n",
    "image_id = 1800\n",
    "\n",
    "# retrieve image exhibiting the image id\n",
    "cifar10_train_data[image_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U7Cj4FeDWFBm"
   },
   "source": [
    "Ok, that doesn't seem easily interpretable ;) Let's first seperate the image from its label information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "sCu10sqhWFBm"
   },
   "outputs": [],
   "source": [
    "cifar10_train_image, cifar10_train_label = cifar10_train_data[image_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a-7VW-RgWFBm"
   },
   "source": [
    "Now, let's inspect the image dimensionality of the sample image: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "aWmU-bu6WFBm"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cifar10_train_image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UtQvpLSTWFBn"
   },
   "source": [
    "Great, we observe that each image corresponds to a three dimensional tensor of size $3*32*32$. Pls. note again, that the first dimension of the tensor correspond to the three RGB (red, green, and blue) color channels of the image. In a next step let's also visually inspect our sample image: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "zG0SKMFEWFBn"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# define tensor to image transformation\n",
    "trans = torchvision.transforms.ToPILImage()\n",
    "\n",
    "# set image plot title \n",
    "plt.title('Example: {}, Label: \"{}\"'.format(str(image_id), str(cifar10_classes[cifar10_train_label])))\n",
    "\n",
    "# un-normalize cifar 10 image sample\n",
    "cifar10_train_image_plot = cifar10_train_image / 5 + 0.5\n",
    "\n",
    "# plot 10 image sample\n",
    "plt.imshow(trans(cifar10_train_image_plot));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LW0bW6OCWFBn"
   },
   "source": [
    "Fantastic, right? Let's now decide on where we want to store the evaluation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "J6LTOUsdWFBn"
   },
   "outputs": [],
   "source": [
    "eval_path = './data/eval_cifar10'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fyS5Ng2OWFBo"
   },
   "source": [
    "And download the evaluation data accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "VBkqztkIWFBo"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/eval_cifar10/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "170500096it [03:03, 901840.34it/s]                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/eval_cifar10/cifar-10-python.tar.gz to ./data/eval_cifar10\n"
     ]
    }
   ],
   "source": [
    "# define pytorch transformation into tensor format\n",
    "transf = torchvision.transforms.Compose([torchvision.transforms.ToTensor(), torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "# download and transform validation images\n",
    "cifar10_eval_data = torchvision.datasets.CIFAR10(root=eval_path, train=False, transform=transf, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xp7zogDcWFBo"
   },
   "source": [
    "Verify the volume of validation images downloaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "qX6XLRBSWFBp"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "170500096it [03:20, 901840.34it/s]"
     ]
    }
   ],
   "source": [
    "# get the length of the training data\n",
    "len(cifar10_eval_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uC9Xzaf0WFBp"
   },
   "source": [
    "## 4. Neural Network Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9wjqpWAsWFBp"
   },
   "source": [
    "In this section we, will implement the architecture of the **neural network** we aim to utilize to learn a model that is capable of classifying the 32x32 pixel CIFAR 10 images according to the objects contained in each image. However, before we start the implementation, let's briefly revisit the process to be established. The following cartoon provides a birds-eye view:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5WU3YnCvWFBp"
   },
   "source": [
    "<img align=\"center\" style=\"max-width: 900px\" src=\"https://github.com/HSG-AIML/LabAIML/blob/main/lab_05/process.png?raw=1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oVVPho0SWFBq"
   },
   "source": [
    "Our CNN, which we name 'CIFAR10Net' and aim to implement consists of two **convolutional layers** and three **fully-connected layers**. In general, convolutional layers are specifically designed to learn a set of **high-level features** (\"patterns\") in the processed images, e.g., tiny edges and shapes. The fully-connected layers utilize the learned features to learn **non-linear feature combinations** that allow for highly accurate classification of the image content into the different image classes of the CIFAR-10 dataset, such as, birds, aeroplanes, horses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vREdAcUbWFBq"
   },
   "source": [
    "Let's implement the network architecture and subsequently have a more in-depth look into its architectural details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "5bLDbE3tWFBr"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "170500096it [04:42, 604086.82it/s]\n"
     ]
    }
   ],
   "source": [
    "# implement the CIFAR10Net network architecture\n",
    "class CIFAR10Net(nn.Module):\n",
    "    \n",
    "    # define the class constructor\n",
    "    def __init__(self):\n",
    "        \n",
    "        # call super class constructor\n",
    "        super(CIFAR10Net, self).__init__()\n",
    "        \n",
    "        # specify convolution layer 1\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5, stride=1, padding=0)\n",
    "        \n",
    "        # define max-pooling layer 1\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # specify convolution layer 2\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1, padding=0)\n",
    "        \n",
    "        # define max-pooling layer 2\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # specify fc layer 1 - in 16 * 5 * 5, out 120\n",
    "        self.linear1 = nn.Linear(16 * 5 * 5, 120, bias=True) # the linearity W*x+b\n",
    "        self.relu1 = nn.ReLU(inplace=True) # the non-linearity\n",
    "        \n",
    "        # specify fc layer 2 - in 120, out 84\n",
    "        self.linear2 = nn.Linear(120, 84, bias=True) # the linearity W*x+b\n",
    "        self.relu2 = nn.ReLU(inplace=True) # the non-linarity\n",
    "        \n",
    "        # specify fc layer 3 - in 84, out 10\n",
    "        self.linear3 = nn.Linear(84, 10) # the linearity W*x+b\n",
    "        \n",
    "        # add a softmax to the last layer\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1) # the softmax\n",
    "        \n",
    "    # define network forward pass\n",
    "    def forward(self, images):\n",
    "        \n",
    "        # high-level feature learning via convolutional layers\n",
    "        \n",
    "        # define conv layer 1 forward pass\n",
    "        x = self.pool1(self.conv1(images))\n",
    "        \n",
    "        # define conv layer 2 forward pass\n",
    "        x = self.pool2(self.conv2(x))\n",
    "        \n",
    "        # feature flattening\n",
    "        \n",
    "        # reshape image pixels\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        \n",
    "        # combination of feature learning via non-linear layers\n",
    "        \n",
    "        # define fc layer 1 forward pass\n",
    "        x = self.relu1(self.linear1(x))\n",
    "        \n",
    "        # define fc layer 2 forward pass\n",
    "        x = self.relu2(self.linear2(x))\n",
    "        \n",
    "        # define layer 3 forward pass\n",
    "        x = self.logsoftmax(self.linear3(x))\n",
    "        \n",
    "        # return forward pass result\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tteiv-LQWFBr"
   },
   "source": [
    "You may have noticed that we applied two more layers (compared to the MNIST example described in the last lab) before the fully-connected layers. These layers are referred to as **convolutional** layers and are usually comprised of three operations, (1) **convolution**, (2) **non-linearity**, and (3) **max-pooling**. Those operations are usually executed in sequential order during the forward pass through a convolutional layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jGF556z4WFBs"
   },
   "source": [
    "In the following, we will have a detailed look into the functionality and number of parameters in each layer. We will start with providing images of 3x32x32 dimensions to the network, i.e., the three channels (red, green, blue) of an image each of size 32x32 pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7R51MsarWFBs"
   },
   "source": [
    "### 4.1. High-Level Feature Learning by Convolutional Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kIHhxd6iWFBt"
   },
   "source": [
    "Let's first have a look into the convolutional layers of the network as illustrated in the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hwnlmr2_WFBt"
   },
   "source": [
    "<img align=\"center\" style=\"max-width: 600px\" src=\"https://github.com/HSG-AIML/LabAIML/blob/main/lab_05/convolutions.png?raw=1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SGLBOsCVWFBu"
   },
   "source": [
    "**First Convolutional Layer**: The first convolutional layer expects three input channels and will convolve six filters each of size 3x5x5. Let's briefly revisit how we can perform a convolutional operation on a given image. For that, we need to define a kernel which is a matrix of size 5x5, for example. To perform the convolution operation, we slide the kernel along with the image horizontally and vertically and obtain the dot product of the kernel and the pixel values of the image inside the kernel ('receptive field' of the kernel)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eB-qZAezWFBu"
   },
   "source": [
    "The following illustration shows an example of a discrete convolution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8oT8YqwnWFBv"
   },
   "source": [
    "<img align=\"center\" style=\"max-width: 800px\" src=\"https://github.com/HSG-AIML/LabAIML/blob/main/lab_05/convsample.png?raw=1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FUpjOZNSWFBv"
   },
   "source": [
    "The left grid is called the input (an image or feature map). The middle grid, referred to as kernel, slides across the input feature map (or image). At each location, the product between each element of the kernel and the input element it overlaps is computed, and the results are summed up to obtain the output in the current location. In general, a discrete convolution is mathematically expressed by:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HxATZuaXWFBv"
   },
   "source": [
    "<center> $y(m, n) = x(m, n) * h(m, n) = \\sum^{m}_{j=0} \\sum^{n}_{i=0} x(i, j) * h(m-i, n-j)$, </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3fcDtXybWFBv"
   },
   "source": [
    "where $x$ denotes the input image or feature map, $h$ the applied kernel, and, $y$ the output. When performing the convolution operation the 'stride' defines the number of pixels to pass at a time when sliding the kernel over the input. While 'padding' adds the number of pixels to the input image (or feature map) to ensure that the output has the same shape as the input. Let's have a look at another animated example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U54J69gtWFBw"
   },
   "source": [
    "<img align=\"center\" style=\"max-width: 800px\" src=\"https://github.com/HSG-AIML/LabAIML/blob/main/lab_05/convsample_animated.gif?raw=1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rFQ0lhjoWFBw"
   },
   "source": [
    "(Source: https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53)\n",
    "\n",
    "In our implementation padding is set to 0 and stride is set to 1. As a result, the output size of the convolutional layer becomes 6x28x28, because (32 - 5) + 1 = 28. This layer exhibits ((5 x 5 x 3) + 1) x 6 = 456 parameter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hde2P9jZWFBw"
   },
   "source": [
    "**First Max-Pooling Layer:** The max-pooling process is a sample-based discretization operation. The objective is to down-sample an input representation (image, hidden-layer output matrix, etc.), reducing its dimensionality and allowing for assumptions to be made about features contained in the sub-regions binned.\n",
    "\n",
    "To conduct such an operation, we again need to define a kernel. Max-pooling kernels are usually a tiny matrix of, e.g, of size 2x2. To perform the max-pooling operation, we slide the kernel along the image horizontally and vertically (similarly to a convolution) and compute the maximum pixel value of the image (or feature map) inside the kernel (the receptive field of the kernel)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZjBY4mNfWFBw"
   },
   "source": [
    "The following illustration shows an example of a max-pooling operation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UFBtAYeHWFBw"
   },
   "source": [
    "<img align=\"center\" style=\"max-width: 500px\" src=\"https://github.com/HSG-AIML/LabAIML/blob/main/lab_05/poolsample.png?raw=1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oOBAAeyvWFBw"
   },
   "source": [
    "The left grid is called the input (an image or feature map). The middle grid, referred to as kernel, slides across the input feature map (or image). We use a stride of 2, meaning the step distance for stepping over our input will be 2 pixels and won't overlap regions. At each location, the max value of the region that overlaps with the elements of the kernel and the input elements it overlaps is computed, and the results are obtained in the output of the current location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A7DQTBGTWFBx"
   },
   "source": [
    "In our implementation, we do max-pooling with a 2x2 kernel and stride 2 this effectively drops the original image size from 6x28x28 to 6x14x14. Let's have a look at an exemplary visualization of 64 features learnt in the first convolutional layer on the CIFAR- 10 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AaZw28DgWFBx"
   },
   "source": [
    "<img align=\"center\" style=\"max-width: 600px\" src=\"https://github.com/HSG-AIML/LabAIML/blob/main/lab_05/cnnfeatures.png?raw=1\">\n",
    "\n",
    "(Source: Yu, Dingjun, Hanli Wang, Peiqiu Chen, and Zhihua Wei. **\"Mixed pooling for convolutional neural networks.\"** In International conference on rough sets and knowledge technology, pp. 364-375. Springer, Cham, 2014)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VWZP8vFmWFBx"
   },
   "source": [
    "**Second Convolutional Layer:** The second convolutional layer expects 6 input channels and will convolve 16 filters each of size 6x5x5x. Since padding is set to 0 and stride is set 1, the output size is 16x10x10, because (14  - 5) + 1 = 10. This layer therefore has ((5 x 5 x 6) + 1 x 16) = 24,16 parameter.\n",
    "\n",
    "**Second Max-Pooling Layer:** The second down-sampling layer uses max-pooling with 2x2 kernel and stride set to 2. This effectively drops the size from 16x10x10 to 16x5x5. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vZZOhRF4WFBx"
   },
   "source": [
    "### 4.2. Feature Flattening\n",
    "\n",
    "The output of the final-max pooling layer needs to be flattened so that we can connect it to a fully connected layer. This is achieved using the `torch.Tensor.view` method. Setting the parameter of the method to `-1` will automatically infer the number of rows required to handle the mini-batch size of the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SI7inpzrWFBx"
   },
   "source": [
    "### 4.3. Learning of Feature Combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JQ7L2q20WFBy"
   },
   "source": [
    "Let's now have a look into the non-linear layers of the network illustrated in the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-d4oMPesWFBy"
   },
   "source": [
    "<img align=\"center\" style=\"max-width: 600px\" src=\"https://github.com/HSG-AIML/LabAIML/blob/main/lab_05/fullyconnected.png?raw=1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PT-uMSIVWFBy"
   },
   "source": [
    "The first fully connected layer uses 'Rectified Linear Units' (ReLU) activation functions to learn potential nonlinear combinations of features. The layers are implemented similarly to the fifth lab. Therefore, we will only focus on the number of parameters of each fully-connected layer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i3W570wwWFBy"
   },
   "source": [
    "**First Fully-Connected Layer:** The first fully-connected layer consists of 120 neurons, thus in total exhibits ((16 x 5 x 5) + 1) x 120 = 48,120 parameter. \n",
    "\n",
    "**Second Fully-Connected Layer:** The output of the first fully-connected layer is then transferred to second fully-connected layer. The layer consists of 84 neurons equipped with ReLu activation functions, this in total exhibits (120 + 1) x 84 = 10,164 parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pB8vsv2KWFBz"
   },
   "source": [
    "The output of the second fully-connected layer is then transferred to the output-layer (third fully-connected layer). The output layer is equipped with a softmax (that you learned about in the previous lab 05) and is made up of ten neurons, one for each object class contained in the CIFAR-10 dataset. This layer exhibits (84 + 1) x 10 = 850 parameter.\n",
    "\n",
    "\n",
    "As a result our CIFAR-10 convolutional neural exhibits a total of 456 + 2,416 + 48,120 + 10,164 + 850 = 62,006 parameter.\n",
    "\n",
    "(Source: https://www.stefanfiott.com/machine-learning/cifar-10-classifier-using-cnn-in-pytorch/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LBOmgsT8WFBz"
   },
   "source": [
    "Now, that we have implemented our first Convolutional Neural Network we are ready to instantiate a network model to be trained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "VQ10HUntWFB0"
   },
   "outputs": [],
   "source": [
    "model = CIFAR10Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UClhzQyWWFB0"
   },
   "source": [
    "Once the model is initialized we can visualize the model structure and review the implemented network architecture by execution of the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "BYYJc0FDWFB0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] CIFAR10Net architecture:\n",
      "\n",
      "CIFAR10Net(\n",
      "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (linear1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (relu1): ReLU(inplace=True)\n",
      "  (linear2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (relu2): ReLU(inplace=True)\n",
      "  (linear3): Linear(in_features=84, out_features=10, bias=True)\n",
      "  (logsoftmax): LogSoftmax(dim=1)\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print the initialized architectures\n",
    "print('[LOG] CIFAR10Net architecture:\\n\\n{}\\n'.format(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "df7m1FscWFB2"
   },
   "source": [
    "Looks like intended? Brilliant! Finally, let's have a look into the number of model parameters that we aim to train in the next steps of the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "0xNNCX_lWFB2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] Number of to be trained CIFAR10Net model parameters: 62006.\n"
     ]
    }
   ],
   "source": [
    "# init the number of model parameters\n",
    "num_params = 0\n",
    "\n",
    "# iterate over the distinct parameters\n",
    "for param in model.parameters():\n",
    "\n",
    "    # collect number of parameters\n",
    "    num_params += param.numel()\n",
    "    \n",
    "# print the number of model paramters\n",
    "print('[LOG] Number of to be trained CIFAR10Net model parameters: {}.'.format(num_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S7owjnXyWFB3"
   },
   "source": [
    "Ok, our \"simple\" CIFAR10Net model already encompasses an impressive number 62'006 model parameters to be trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5r-Tr9PsWFB3"
   },
   "source": [
    "Now that we have implemented the CIFAR10Net, we are ready to train the network. However, before starting the training, we need to define an appropriate loss function. Remember, we aim to train our model to learn a set of model parameters $\\theta$ that minimize the classification error of the true class $c^{i}$ of a given CIFAR-10 image $x^{i}$ and its predicted class $\\hat{c}^{i} = f_\\theta(x^{i})$ as faithfully as possible. \n",
    "\n",
    "In this lab we use (similarly to lab 05) the **'Negative Log-Likelihood (NLL)'** loss. During training the NLL loss will penalize models that result in a high classification error between the predicted class labels $\\hat{c}^{i}$ and their respective true class label $c^{i}$. Now that we have implemented the CIFAR10Net, we are ready to train the network. Before starting the training, we need to define an appropriate loss function. Remember, we aim to train our model to learn a set of model parameters $\\theta$ that minimize the classification error of the true class $c^{i}$ of a given CIFAR-10 image $x^{i}$ and its predicted class $\\hat{c}^{i} = f_\\theta(x^{i})$ as faithfully as possible. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BPfoNHduWFB3"
   },
   "source": [
    "Let's instantiate the NLL via the execution of the following PyTorch command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "iAlcIntTWFB4"
   },
   "outputs": [],
   "source": [
    "# define the optimization criterion / loss function\n",
    "nll_loss = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jD6DGTuCWFB4"
   },
   "source": [
    "Based on the loss magnitude of a certain mini-batch PyTorch automatically computes the gradients. But even better, based on the gradient, the library also helps us in the optimization and update of the network parameters $\\theta$.\n",
    "\n",
    "We will use the **Stochastic Gradient Descent (SGD) optimization** and set the `learning-rate to 0.001`. Each mini-batch step the optimizer will update the model parameters $\\theta$ values according to the degree of classification error (the NLL loss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "Kv3hsPxlWFB5"
   },
   "outputs": [],
   "source": [
    "# define learning rate and optimization strategy\n",
    "learning_rate = 0.001\n",
    "optimizer = optim.SGD(params=model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v-GdZ3mtWFB5"
   },
   "source": [
    "Now that we have successfully implemented and defined the three CNN building blocks let's take some time to review the `CIFAR10Net` model definition as well as the `loss`. Please, read the above code and comments carefully and don't hesitate to let us know any questions you might have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hLxKfviTWFB5"
   },
   "source": [
    "## 5. Training the Neural Network Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eI9GIT-kWFB5"
   },
   "source": [
    "In this section, we will train our neural network model (as implemented in the section above) using the transformed images. More specifically, we will have a detailed look into the distinct training steps as well as how to monitor the training progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u4zXSa9kWFB6"
   },
   "source": [
    "### 5.1. Preparing the Network Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wNT1eQ4-WFB6"
   },
   "source": [
    "So far, we have pre-processed the dataset, implemented the CNN and defined the classification error. Let's now start to train a corresponding model for **20 epochs** and a **mini-batch size of 4** CIFAR-10 images per batch. This implies that the whole dataset will be fed to the CNN 20 times in chunks of 4 images yielding to **12,500 mini-batches** (50.000 training images / 4 images per mini-batch) per epoch. After the processing of each mini-batch, the parameters of the network will be updated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "HI2ZQOQ8WFB6"
   },
   "outputs": [],
   "source": [
    "# specify the training parameters\n",
    "num_epochs = 20 # number of training epochs\n",
    "mini_batch_size = 4 # size of the mini-batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wk8FEox2WFB6"
   },
   "source": [
    "Furthermore, lets specifiy and instantiate a corresponding PyTorch data loader that feeds the image tensors to our neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "TRP3TtTxWFB7"
   },
   "outputs": [],
   "source": [
    "cifar10_train_dataloader = torch.utils.data.DataLoader(cifar10_train_data, batch_size=mini_batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HxvNSTZOWFB7"
   },
   "source": [
    "### 5.2. Running the Network Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j1fs3QJCWFB7"
   },
   "source": [
    "Finally, we start training the model. The training procedure for each mini-batch is performed as follows: \n",
    "\n",
    ">1. do a forward pass through the CIFAR10Net network, \n",
    ">2. compute the negative log-likelihood classification error $\\mathcal{L}^{NLL}_{\\theta}(c^{i};\\hat{c}^{i})$, \n",
    ">3. do a backward pass through the CIFAR10Net network, and \n",
    ">4. update the parameters of the network $f_\\theta(\\cdot)$.\n",
    "\n",
    "To ensure learning while training our CNN model, we will monitor whether the loss decreases with progressing training. Therefore, we obtain and evaluate the classification performance of the entire training dataset after each training epoch. Based on this evaluation, we can conclude on the training progress and whether the loss is converging (indicating that the model might not improve any further).\n",
    "\n",
    "The following elements of the network training code below should be given particular attention:\n",
    " \n",
    ">- `loss.backward()` computes the gradients based on the magnitude of the reconstruction loss,\n",
    ">- `optimizer.step()` updates the network parameters based on the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "9ek4RxRFWFB7",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20211206-12:52:22] epoch: 0 train-loss: 1.947801118798256\n",
      "[LOG 20211206-12:52:58] epoch: 1 train-loss: 1.5709528248465061\n",
      "[LOG 20211206-12:53:34] epoch: 2 train-loss: 1.4168253201019765\n",
      "[LOG 20211206-12:54:10] epoch: 3 train-loss: 1.3247313293468952\n",
      "[LOG 20211206-12:55:12] epoch: 4 train-loss: 1.2531105115568637\n",
      "[LOG 20211206-12:57:39] epoch: 5 train-loss: 1.190221012313664\n",
      "[LOG 20211206-12:59:13] epoch: 6 train-loss: 1.1344387248903514\n",
      "[LOG 20211206-13:01:29] epoch: 7 train-loss: 1.0822005566293\n",
      "[LOG 20211206-13:03:54] epoch: 8 train-loss: 1.0390859489947557\n",
      "[LOG 20211206-13:04:40] epoch: 9 train-loss: 0.9997282166466117\n",
      "[LOG 20211206-13:05:17] epoch: 10 train-loss: 0.9623042580966652\n",
      "[LOG 20211206-13:06:12] epoch: 11 train-loss: 0.9284629686282575\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-adc373d00459>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# update network paramaters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m# collect mini-batch reconstruction loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/anaconda3/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/anaconda3/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/anaconda3/lib/python3.7/site-packages/torch/optim/sgd.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    115\u001b[0m                   \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                   \u001b[0mdampening\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdampening\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                   nesterov=nesterov)\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;31m# update momentum_buffers in state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/anaconda3/lib/python3.7/site-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36msgd\u001b[0;34m(params, d_p_list, momentum_buffer_list, weight_decay, momentum, lr, dampening, nesterov)\u001b[0m\n\u001b[1;32m    178\u001b[0m                 \u001b[0md_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m         \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# init collection of training epoch losses\n",
    "train_epoch_losses = []\n",
    "\n",
    "# set the model in training mode\n",
    "model.train()\n",
    "\n",
    "# train the CIFAR10 model\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # init collection of mini-batch losses\n",
    "    train_mini_batch_losses = []\n",
    "    \n",
    "    # iterate over all-mini batches\n",
    "    for i, (images, labels) in enumerate(cifar10_train_dataloader):\n",
    "        \n",
    "        # run forward pass through the network\n",
    "        output = model(images)\n",
    "        \n",
    "        # reset graph gradients\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # determine classification loss\n",
    "        loss = nll_loss(output, labels)\n",
    "        \n",
    "        # run backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # update network paramaters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # collect mini-batch reconstruction loss\n",
    "        train_mini_batch_losses.append(loss.data.item())\n",
    "\n",
    "    # determine mean min-batch loss of epoch\n",
    "    train_epoch_loss = np.mean(train_mini_batch_losses)\n",
    "    \n",
    "    # print epoch loss\n",
    "    now = datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
    "    print('[LOG {}] epoch: {} train-loss: {}'.format(str(now), str(epoch), str(train_epoch_loss)))\n",
    "    \n",
    "    # save model to local directory\n",
    "    model_name = 'cifar10_model_epoch_{}.pth'.format(str(epoch))\n",
    "    torch.save(model.state_dict(), os.path.join(\"./models\", model_name))\n",
    "    \n",
    "    # determine mean min-batch loss of epoch\n",
    "    train_epoch_losses.append(train_epoch_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xu-RS14lWFB8"
   },
   "source": [
    "Upon successfull training let's visualize and inspect the training loss per epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PerkKkQzWFB8"
   },
   "outputs": [],
   "source": [
    "# prepare plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# add grid\n",
    "ax.grid(linestyle='dotted')\n",
    "\n",
    "# plot the training epochs vs. the epochs' classification error\n",
    "ax.plot(np.array(range(1, len(train_epoch_losses)+1)), train_epoch_losses, label='epoch loss (blue)')\n",
    "\n",
    "# add axis legends\n",
    "ax.set_xlabel(\"[training epoch $e_i$]\", fontsize=10)\n",
    "ax.set_ylabel(\"[Classification Error $\\mathcal{L}^{NLL}$]\", fontsize=10)\n",
    "\n",
    "# set plot legend\n",
    "plt.legend(loc=\"upper right\", numpoints=1, fancybox=True)\n",
    "\n",
    "# add plot title\n",
    "plt.title('Training Epochs $e_i$ vs. Classification Error $L^{NLL}$', fontsize=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "85nXpiEZWFB8"
   },
   "source": [
    "Ok, fantastic. The training error converges nicely. We could definitely train the network a couple more epochs until the error converges. But let's stay with the 20 training epochs for now and continue with evaluating our trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yhyqRcAdWFB9"
   },
   "source": [
    "## 6. Evaluation of the Trained Neural Network Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k_b43DckWFB9"
   },
   "source": [
    "Prior to evaluating our model, let's load the best performing model. Remember, that we stored a snapshot of the model after each training epoch to our local model directory. We will now load the last snapshot saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "2z8SofRtWFB9"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "unknown url type: 'models/cifar10_model_epoch_19.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-3d1598e2e434>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# read stored model from the remote location\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmodel_bytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_model_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# load model tensor from io.BytesIO object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/anaconda3/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/anaconda3/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    508\u001b[0m         \u001b[0;31m# accept a URL or a Request object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfullurl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m             \u001b[0mreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfullurl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    511\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m             \u001b[0mreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfullurl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/anaconda3/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, url, data, headers, origin_req_host, unverifiable, method)\u001b[0m\n\u001b[1;32m    326\u001b[0m                  \u001b[0morigin_req_host\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munverifiable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m                  method=None):\n\u001b[0;32m--> 328\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munredirected_hdrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/anaconda3/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mfull_url\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_full_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_full_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfragment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplittag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_full_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeleter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/anaconda3/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36m_parse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplittype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_full_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"unknown url type: %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplithost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: unknown url type: 'models/cifar10_model_epoch_19.pth'"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "import io\n",
    "\n",
    "# restore pre-trained model snapshot\n",
    "best_model_name = 'https://raw.githubusercontent.com/HSG-AIML/LabAIML/master/lab_05/models/cifar10_model_epoch_19.pth'\n",
    "\n",
    "# read stored model from the remote location\n",
    "model_bytes = urllib.request.urlopen(best_model_name)\n",
    "\n",
    "# load model tensor from io.BytesIO object\n",
    "model_buffer = io.BytesIO(model_bytes.read())\n",
    "\n",
    "# init pre-trained model class\n",
    "best_model = CIFAR10Net()\n",
    "\n",
    "# load pre-trained models\n",
    "best_model.load_state_dict(torch.load(model_buffer, map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wzVhI35JWFB9"
   },
   "source": [
    "Let's inspect if the model was loaded successfully: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8OZjQIgVWFB-"
   },
   "outputs": [],
   "source": [
    "# set model in evaluation mode\n",
    "best_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E1-2BEIuWFB-"
   },
   "source": [
    "In order to evaluate our trained model, we need to feed the CIFAR10 images reserved for evaluation (the images that we didn't use as part of the training process) through the model. Therefore, let's again define a corresponding PyTorch data loader that feeds the image tensors to our neural network: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CejfY-0CWFB_"
   },
   "outputs": [],
   "source": [
    "cifar10_eval_dataloader = torch.utils.data.DataLoader(cifar10_eval_data, batch_size=10000, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g57Od_qbWFCA"
   },
   "source": [
    "We will now evaluate the trained model using the same mini-batch approach as we did when training the network and derive the mean negative log-likelihood loss of all mini-batches processed in an epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8LRy_do8WFCA"
   },
   "outputs": [],
   "source": [
    "# init collection of mini-batch losses\n",
    "eval_mini_batch_losses = []\n",
    "\n",
    "# iterate over all-mini batches\n",
    "for i, (images, labels) in enumerate(cifar10_eval_dataloader):\n",
    "    \n",
    "    # run forward pass through the network\n",
    "    output = best_model(images)\n",
    "\n",
    "    # determine classification loss\n",
    "    loss = nll_loss(output, labels)\n",
    "\n",
    "    # collect mini-batch reconstruction loss\n",
    "    eval_mini_batch_losses.append(loss.data.item())\n",
    "\n",
    "# determine mean min-batch loss of epoch\n",
    "eval_loss = np.mean(eval_mini_batch_losses)\n",
    "\n",
    "# print epoch loss\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
    "print('[LOG {}] eval-loss: {}'.format(str(now), str(eval_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gbnhjq9cWFCA"
   },
   "source": [
    "Ok, great. The evaluation loss looks in-line with our training loss. Let's now inspect a few sample predictions to get an impression of the model quality. Therefore, we will again pick a random image of our evaluation dataset and retrieve its PyTorch tensor as well as the corresponding label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EQS5iLnSWFCB"
   },
   "outputs": [],
   "source": [
    "# set (random) image id\n",
    "image_id = 777\n",
    "\n",
    "# retrieve image exhibiting the image id\n",
    "cifar10_eval_image, cifar10_eval_label = cifar10_eval_data[image_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EqWZfIM1WFCB"
   },
   "source": [
    "Let's now inspect the true class of the image we selected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yBnV4bx6WFCB"
   },
   "outputs": [],
   "source": [
    "cifar10_classes[cifar10_eval_label]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ma9AeElxWFCC"
   },
   "source": [
    "Ok, the randomly selected image should contain a two (2). Let's inspect the image accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B7BNai4xWFCC"
   },
   "outputs": [],
   "source": [
    "# define tensor to image transformation\n",
    "trans = torchvision.transforms.ToPILImage()\n",
    "\n",
    "# set image plot title \n",
    "plt.title('Example: {}, Label: {}'.format(str(image_id), str(cifar10_classes[cifar10_eval_label])))\n",
    "\n",
    "# un-normalize cifar 10 image sample\n",
    "cifar10_eval_image_plot = cifar10_eval_image / 2.0 + 0.5\n",
    "\n",
    "# plot cifar 10 image sample\n",
    "plt.imshow(trans(cifar10_eval_image_plot));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B3qfEzUyWFCD"
   },
   "source": [
    "Ok, let's compare the true label with the prediction of our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2SgBYH5XWFCF"
   },
   "outputs": [],
   "source": [
    "best_model(cifar10_eval_image.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ei6TYZuDWFCG"
   },
   "source": [
    "We can even determine the likelihood of the most probable class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V5LbE7DeWFCG"
   },
   "outputs": [],
   "source": [
    "cifar10_classes[torch.argmax(best_model(Variable(cifar10_eval_image.unsqueeze(0))), dim=1).item()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zwfXD45qWFCG"
   },
   "source": [
    "Let's now obtain the predictions for all the CIFAR-10 images of the evaluation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ars-f77YWFCG"
   },
   "outputs": [],
   "source": [
    "predictions = torch.argmax(best_model(iter(cifar10_eval_dataloader).next()[0]), dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KhGTINTlWFCH"
   },
   "source": [
    "Furthermore, let's obtain the overall classification accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pcjzWzObWFCH"
   },
   "outputs": [],
   "source": [
    "metrics.accuracy_score(cifar10_eval_data.targets, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bvm7gkp8WFCH"
   },
   "source": [
    "Let's also inspect the confusion matrix of the model predictions to determine major sources of misclassification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uSZHJnylWFCH"
   },
   "outputs": [],
   "source": [
    "# determine classification matrix of the predicted and target classes\n",
    "mat = confusion_matrix(cifar10_eval_data.targets, predictions)\n",
    "\n",
    "# plot corresponding confusion matrix\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False, cmap='YlOrRd_r', xticklabels=cifar10_classes, yticklabels=cifar10_classes)\n",
    "\n",
    "# add the confusion matrix title\n",
    "plt.title('CIFAR-10 classification matrix')\n",
    "\n",
    "# add the x-axis label\n",
    "plt.xlabel('[true label]')\n",
    "\n",
    "# add the y-axis label\n",
    "plt.ylabel('[predicted label]');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GluVCdz8WFCH"
   },
   "source": [
    "Ok, we can easily see that our current model confuses images of cats and dogs as well as images of trucks and cars quite often. This is again not surprising since those image categories exhibit a high semantic and therefore visual similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BinN3SFcWFCI"
   },
   "source": [
    "### Exercises:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iwe6m8IUWFCI"
   },
   "source": [
    "We recommend you try the following exercises as part of the lab:\n",
    "\n",
    "**1. Train the network a couple more epochs and evaluate its prediction accuracy.**\n",
    "\n",
    "> Increase the number of training epochs up to 50 epochs and re-run the network training. Load and evaluate the model exhibiting the lowest training loss. What kind of behavior in terms of prediction accuracy can be observed with increasing the training epochs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YCG1eBdYWFCI"
   },
   "outputs": [],
   "source": [
    "# ***************************************************\n",
    "# INSERT YOUR CODE HERE\n",
    "# ***************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "41Ifk9DVWFCI"
   },
   "source": [
    "**2. Evaluation of \"shallow\" vs. \"deep\" neural network architectures.**\n",
    "\n",
    "> In addition to the architecture of the lab notebook, evaluate further (more shallow as well as more deep) neural network architectures by (1) either removing or adding layers to the network and/or (2) increasing/decreasing the number of neurons per layer. Train a model (using the architectures you selected) for at least 50 training epochs. Analyze the prediction performance of the trained models in terms of training time and prediction accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lZQRMPaUWFCI"
   },
   "outputs": [],
   "source": [
    "# ***************************************************\n",
    "# INSERT YOUR CODE HERE\n",
    "# ***************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N7441DlkWFCJ"
   },
   "source": [
    "### Lab Summary:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RNBr4BP7WFCJ"
   },
   "source": [
    "In this lab, a step by step introduction into **design, implementation, training and evaluation** of convolutional neural networks CNNs to classify tiny images of objects is presented. The code and exercises presented in this lab may serves as a starting point for developing more complex, deeper and more tailored CNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IXeX3F43WFCJ"
   },
   "source": [
    "You may want to execute the content of your lab outside of the Jupyter notebook environment, e.g. on a compute node or a server. The cell below converts the lab notebook into a standalone and executable python script. Pls. note that to convert the notebook, you need to install Python's **nbconvert** library and its extensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zCXKJU1SWFCJ"
   },
   "outputs": [],
   "source": [
    "# installing the nbconvert library\n",
    "!pip install nbconvert\n",
    "!pip install jupyter_contrib_nbextensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iaRdr_N-WFCK"
   },
   "source": [
    "Let's now convert the Jupyter notebook into a plain Python script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g8BWdVwOWFCK"
   },
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to script aiml_lab_05.ipynb"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "8YfYnXvNWFBV",
    "0NlCbmDLWFBY",
    "AMwNx9eOWFBe",
    "uC9Xzaf0WFBp",
    "7R51MsarWFBs",
    "vZZOhRF4WFBx",
    "hLxKfviTWFB5",
    "yhyqRcAdWFB9",
    "BinN3SFcWFCI"
   ],
   "name": "aiml_lab_06.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "8a759edab9623fec557173fc5dc3172aac588fd51e2e191f985f3e24b521fb85"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "393.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
